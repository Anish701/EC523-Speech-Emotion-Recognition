{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JngnQaoBmIzK"
   },
   "source": [
    "# **Speech to Emotion Recognition - Mamba Testing**\n",
    "James Knee, Tyler Nguyen, Varsha Singh, Anish Sinha, Nathan Strahs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFAKF_NON0U7"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/ec523/projects/teamSER/miniconda/envs/mamba-env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MKmygLkSNuoy"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "import random\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from mamba_ssm import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  9 19:02:39 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  |   00000000:18:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             43W /  300W |       4MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-16GB           On  |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             43W /  300W |       1MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-16GB           On  |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   60C    P0            180W /  300W |     536MiB /  16384MiB |     72%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-16GB           On  |   00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   50C    P0             62W /  300W |    1356MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    2   N/A  N/A          756636      C   python                                  532MiB |\n",
      "|    3   N/A  N/A          834727      C   ....10.12/install/bin/python3.10       1352MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mamba', 'Mamba2', 'MambaLMHeadModel', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'distributed', 'mamba_inner_fn', 'models', 'modules', 'ops', 'selective_scan_fn', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import mamba_ssm\n",
    "print(dir(mamba_ssm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING: cuda\n",
      "Mamba output shape: torch.Size([2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"USING: \" + device.type)\n",
    "\n",
    "# Define a tiny Mamba model and move it to the GPU\n",
    "model = Mamba(\n",
    "    d_model=16,  # embedding dimension\n",
    "    d_state=8,   # internal state dimension\n",
    "    d_conv=4,    # convolution dimension\n",
    "    expand=2,    # expansion factor\n",
    ").to(device)\n",
    "\n",
    "# Generate random input (batch_size, sequence_length, embedding_dim) and move it to the GPU\n",
    "x = torch.randn(2, 10, 16).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "print(\"Mamba output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary variables, assuming root directory is /projectnb/ec523/projects/teamSER folder\n",
    "DATA_PATH=\"AudioWAV/\"\n",
    "\n",
    "training_split=0.8\n",
    "testing_split=0.2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=False, target_length=160):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.target_length = target_length\n",
    "\n",
    "        # enumeration of emotions\n",
    "        self.emotion_map = {\n",
    "            \"ANG\": 0, \"DIS\": 1, \"FEA\": 2,\n",
    "            \"HAP\": 3, \"NEU\": 4, \"SAD\": 5\n",
    "        }\n",
    "\n",
    "        # Filter only valid files with known emotion labels\n",
    "        self.audio_files = [\n",
    "            f for f in os.listdir(data_dir)\n",
    "            if f.endswith('.wav') and f.split('_')[2] in self.emotion_map\n",
    "        ]\n",
    "\n",
    "        # Extract labels\n",
    "        self.strlabels = [f.split('_')[2] for f in self.audio_files]\n",
    "        self.labels = [self.emotion_map[label] for label in self.strlabels]\n",
    "\n",
    "        # Fixed transforms\n",
    "        self.sample_rate = 16000\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=2048,\n",
    "            hop_length=512,\n",
    "            n_mels=128\n",
    "        )\n",
    "        self.db_transform = T.AmplitudeToDB()\n",
    "\n",
    "        # Resampler reused for efficiency\n",
    "        self.resampler = T.Resample(orig_freq=48000, new_freq=self.sample_rate)  # Assume worst-case\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.audio_files[idx])\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        # Resample to 16kHz if needed\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resample = T.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "            waveform = resample(waveform)\n",
    "\n",
    "        # Convert stereo to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Normalize waveform\n",
    "        waveform = waveform - waveform.mean()\n",
    "\n",
    "        # Volume augmentation on waveform\n",
    "        if self.transform and random.random() < 0.5:\n",
    "            waveform = T.Vol(gain=(0.5, 1.5), gain_type=\"amplitude\")(waveform)\n",
    "\n",
    "        # Compute Mel spectrogram and convert to dB\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        mel_spec = self.db_transform(mel_spec)\n",
    "\n",
    "        # MinMax normalization to [0, 1]\n",
    "        mel_min = mel_spec.min()\n",
    "        mel_max = mel_spec.max()\n",
    "        mel_spec = (mel_spec - mel_min) / (mel_max - mel_min + 1e-6)\n",
    "\n",
    "        # Spectrogram-level augmentation\n",
    "        if self.transform:\n",
    "            if random.random() < 0.5:\n",
    "                mel_spec = T.FrequencyMasking(freq_mask_param=15)(mel_spec)\n",
    "            if random.random() < 0.5:\n",
    "                mel_spec = T.TimeMasking(time_mask_param=35)(mel_spec)\n",
    "\n",
    "        # Fix time dimension by padding or cropping\n",
    "        current_length = mel_spec.shape[-1]\n",
    "        if current_length < self.target_length:\n",
    "            pad_amount = self.target_length - current_length\n",
    "            mel_spec = F.pad(mel_spec, (0, pad_amount))\n",
    "        else:\n",
    "            mel_spec = mel_spec[:, :, :self.target_length]\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        # Remove channel dimension if needed (1, 128, T) -> (128, T)\n",
    "        mel_spec = mel_spec.squeeze(0)\n",
    "\n",
    "        return mel_spec, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function pads per batch so that every spectogram is the same dimension per batch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    spectrograms, labels = zip(*batch)\n",
    "    \n",
    "    max_length = max(spec.shape[1] for spec in spectrograms)\n",
    "\n",
    "    #pad spectrograms to match longest\n",
    "    spectrograms_padded = [torch.nn.functional.pad(spec, (0, max_length - spec.shape[1])) for spec in spectrograms]\n",
    "\n",
    "    # Convert list to tensor\n",
    "    spectrograms_padded = torch.stack(spectrograms_padded)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return spectrograms_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring dataset\n",
    "dataset = AudioDataset(DATA_PATH)\n",
    "\n",
    "#calculate training size and testing size\n",
    "train_size = int(dataset.__len__()*training_split)\n",
    "test_size = dataset.__len__()-train_size\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_set.dataset.transform = True\n",
    "test_set.dataset.transform = False\n",
    "\n",
    "#dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "#FINAL DIMENSIONS OF SPECS: BatchSize x 128 x MaxTimeLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.7834 | Val Loss: 1.7523 | Val Acc: 17.86%\n",
      "Epoch 2: Train Loss: 1.7337 | Val Loss: 1.7076 | Val Acc: 29.35%\n",
      "Epoch 3: Train Loss: 1.6784 | Val Loss: 1.6335 | Val Acc: 29.21%\n",
      "Epoch 4: Train Loss: 1.5606 | Val Loss: 1.5326 | Val Acc: 36.74%\n",
      "Epoch 5: Train Loss: 1.5035 | Val Loss: 1.4644 | Val Acc: 38.68%\n",
      "Epoch 6: Train Loss: 1.4868 | Val Loss: 1.4381 | Val Acc: 38.48%\n",
      "Epoch 7: Train Loss: 1.4705 | Val Loss: 1.4229 | Val Acc: 41.10%\n",
      "Epoch 8: Train Loss: 1.4516 | Val Loss: 1.4133 | Val Acc: 41.71%\n",
      "Epoch 9: Train Loss: 1.4399 | Val Loss: 1.4246 | Val Acc: 43.92%\n",
      "Epoch 10: Train Loss: 1.4149 | Val Loss: 1.4282 | Val Acc: 43.32%\n",
      "Epoch 11: Train Loss: 1.3965 | Val Loss: 1.3816 | Val Acc: 45.40%\n",
      "Epoch 12: Train Loss: 1.3914 | Val Loss: 1.4011 | Val Acc: 46.34%\n",
      "Epoch 13: Train Loss: 1.3815 | Val Loss: 1.4063 | Val Acc: 44.66%\n",
      "Epoch 14: Train Loss: 1.3729 | Val Loss: 1.3805 | Val Acc: 47.01%\n",
      "Epoch 15: Train Loss: 1.3577 | Val Loss: 1.3751 | Val Acc: 46.88%\n",
      "Epoch 16: Train Loss: 1.3636 | Val Loss: 1.4291 | Val Acc: 47.01%\n",
      "Epoch 17: Train Loss: 1.3547 | Val Loss: 1.3983 | Val Acc: 47.35%\n",
      "Epoch 18: Train Loss: 1.3425 | Val Loss: 1.3754 | Val Acc: 46.34%\n",
      "Epoch 19: Train Loss: 1.3427 | Val Loss: 1.3048 | Val Acc: 48.56%\n",
      "Epoch 20: Train Loss: 1.3315 | Val Loss: 1.3020 | Val Acc: 48.35%\n"
     ]
    }
   ],
   "source": [
    "class PureAudioMamba(nn.Module):  \n",
    "    def __init__(self, num_classes=6, d_model=256):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(128, d_model)\n",
    "        \n",
    "        self.mamba1 = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=16,\n",
    "            d_conv=4,\n",
    "            expand=2\n",
    "        )\n",
    "        self.mamba2 = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=16,\n",
    "            d_conv=4,\n",
    "            expand=2\n",
    "        )\n",
    "        \n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(d_model, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  \n",
    "        x = self.input_proj(x)  \n",
    "        \n",
    "        x = self.mamba1(x)\n",
    "        x = self.mamba2(x)\n",
    "        \n",
    "        attn_weights = self.attention_pool(x)\n",
    "        x = torch.sum(x * attn_weights, dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def train_model(train_loader, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PureAudioMamba(num_classes=6).to(device)  # Fixed class name\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                              max_lr=1e-3,\n",
    "                                              total_steps=len(train_loader)*100)\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for specs, labels in train_loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for specs, labels in test_loader:\n",
    "                specs, labels = specs.to(device), labels.to(device)\n",
    "                outputs = model(specs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "train_model(train_loader, test_loader)\n",
    "\n",
    "# NOTE: mamba uses roughly 3 * expand * d_modeel^2 parameters\n",
    "# NOTE: AdamW is stochastic optimization that modifes the\n",
    "# Adam optimizer by decoupling weight decay from gradient update\n",
    "# NOTE: could also try switching scheduler for cosine with warmup; or lower max_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test how many Epochs is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Train Loss: 1.7303 | Train Acc: 22.12% | Val Loss: 1.6675 | Val Acc: 26.33%\n",
      "Epoch 002: Train Loss: 1.5496 | Train Acc: 33.93% | Val Loss: 1.4819 | Val Acc: 34.45%\n",
      "Epoch 003: Train Loss: 1.5505 | Train Acc: 35.71% | Val Loss: 1.4555 | Val Acc: 37.61%\n",
      "Epoch 004: Train Loss: 1.4787 | Train Acc: 37.44% | Val Loss: 1.4295 | Val Acc: 39.76%\n",
      "Epoch 005: Train Loss: 1.4563 | Train Acc: 39.58% | Val Loss: 1.4077 | Val Acc: 41.97%\n",
      "Epoch 006: Train Loss: 1.4305 | Train Acc: 40.92% | Val Loss: 1.3753 | Val Acc: 43.72%\n",
      "Epoch 007: Train Loss: 1.4165 | Train Acc: 41.95% | Val Loss: 1.3702 | Val Acc: 42.78%\n",
      "Epoch 008: Train Loss: 1.3881 | Train Acc: 43.66% | Val Loss: 1.3334 | Val Acc: 46.54%\n",
      "Epoch 009: Train Loss: 1.3653 | Train Acc: 44.73% | Val Loss: 1.3124 | Val Acc: 47.08%\n",
      "Epoch 010: Train Loss: 1.3493 | Train Acc: 45.39% | Val Loss: 1.3073 | Val Acc: 46.68%\n",
      "Epoch 011: Train Loss: 1.3311 | Train Acc: 46.14% | Val Loss: 1.3354 | Val Acc: 45.94%\n",
      "Epoch 012: Train Loss: 1.3279 | Train Acc: 47.02% | Val Loss: 1.2846 | Val Acc: 48.09%\n",
      "Epoch 013: Train Loss: 1.3086 | Train Acc: 46.97% | Val Loss: 1.2910 | Val Acc: 46.94%\n",
      "Epoch 014: Train Loss: 1.3046 | Train Acc: 47.66% | Val Loss: 1.2716 | Val Acc: 48.89%\n",
      "Epoch 015: Train Loss: 1.2812 | Train Acc: 48.90% | Val Loss: 1.3214 | Val Acc: 46.41%\n",
      "Epoch 016: Train Loss: 1.2842 | Train Acc: 49.44% | Val Loss: 1.2739 | Val Acc: 50.50%\n",
      "Epoch 017: Train Loss: 1.2700 | Train Acc: 49.29% | Val Loss: 1.2367 | Val Acc: 51.85%\n",
      "Epoch 018: Train Loss: 1.2666 | Train Acc: 50.16% | Val Loss: 1.2322 | Val Acc: 51.98%\n",
      "Epoch 019: Train Loss: 1.2563 | Train Acc: 50.66% | Val Loss: 1.2475 | Val Acc: 51.18%\n",
      "Epoch 020: Train Loss: 1.2360 | Train Acc: 50.81% | Val Loss: 1.2058 | Val Acc: 53.39%\n",
      "Epoch 021: Train Loss: 1.2390 | Train Acc: 50.88% | Val Loss: 1.2211 | Val Acc: 51.31%\n",
      "Epoch 022: Train Loss: 1.2227 | Train Acc: 51.79% | Val Loss: 1.2178 | Val Acc: 52.05%\n",
      "Epoch 023: Train Loss: 1.2306 | Train Acc: 51.30% | Val Loss: 1.3212 | Val Acc: 46.94%\n",
      "Epoch 024: Train Loss: 1.2071 | Train Acc: 52.54% | Val Loss: 1.2350 | Val Acc: 50.24%\n",
      "Epoch 025: Train Loss: 1.2023 | Train Acc: 52.33% | Val Loss: 1.2029 | Val Acc: 50.97%\n",
      "No improvement for 5 epochs. Early stopping...\n",
      "Early stopping at epoch 26\n",
      "\n",
      "Training complete. Best validation accuracy: 53.39% at epoch 20\n"
     ]
    }
   ],
   "source": [
    "class PureAudioMamba(nn.Module):  \n",
    "    def __init__(self, num_classes=6, d_model=256):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(128, d_model)\n",
    "        self.mamba1 = Mamba(d_model=d_model, d_state=16, d_conv=4, expand=2)\n",
    "        self.mamba2 = Mamba(d_model=d_model, d_state=16, d_conv=4, expand=2)\n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(d_model, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.mamba1(x)\n",
    "        x = self.mamba2(x)\n",
    "        attn_weights = self.attention_pool(x)\n",
    "        x = torch.sum(x * attn_weights, dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def train_model(train_loader, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PureAudioMamba(num_classes=6).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_epoch = 0\n",
    "    patience = 5  # Stop after 5 epochs without improvement\n",
    "    early_stop = False\n",
    "    \n",
    "    for epoch in range(100):  # Max epochs set high for early stopping\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for specs, labels in train_loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for specs, labels in test_loader:\n",
    "                specs, labels = specs.to(device), labels.to(device)\n",
    "                outputs = model(specs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1:03d}: \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "        elif (epoch + 1 - best_epoch) >= patience:\n",
    "            print(f\"No improvement for {patience} epochs. Early stopping...\")\n",
    "            early_stop = True\n",
    "            \n",
    "    print(f\"\\nTraining complete. Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "\n",
    "# Start training\n",
    "train_model(train_loader, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMd2XnadPDWOhFscMsWNATp",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
