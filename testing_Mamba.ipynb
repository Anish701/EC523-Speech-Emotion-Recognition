{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JngnQaoBmIzK"
   },
   "source": [
    "# **Speech to Emotion Recognition**\n",
    "James Knee, Tyler Nguyen, Varsha Singh, Anish Sinha, Nathan Strahs\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Task ###\n",
    "Our task is to use a deep learning architecture to identify the underlying emotion given some English speaking audio, formally known as Speech to Emotion Recognition (SER). Identifying emotions from speech is hard enough for people, and it requires careful analysis over time. Emotional conveyance is also subjective; different speakers articulate emotions differently, implying variations in pitch, intensity, rhythm, and cadence. This task is also challenging due to the complexity of raw audio signals, so the data will require significant amounts of preprocessing. In the end, we would like our model to differentiate between anger, disgust, fear, happiness, sadness, and neutrality.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roRQN0FsmxCS"
   },
   "source": [
    "# Architecture Overview\n",
    "\n",
    "1. **Preprocessing**\n",
    "      - Normalize audio volume\n",
    "      - Convert audio to time-frequency representations like Spectograms\n",
    "2. **Feature Extraction via ResNet**\n",
    "      - Feed spectogram into Residual Network\n",
    "      - Retain extracted features by removing final classification layer in ResNet\n",
    "3. **Temporal Modeling via Transformer Encoder**\n",
    "      - Pass ResNet output to transformer and capture long-range dependencies and sequential relationships in the audio\n",
    "4. **Classification Layer**\n",
    "      - Apply a softmax layer to classify the output into one of six emotion categories: anger, disgust, fear, happiness, sadness, neutrality.\n",
    "\n",
    "Alternative Model: State Space Model (SSM) such as Mamba\n",
    "\n",
    "[insert diagram here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP5MMN9Gm4ZH"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Below are the datasets we will use for our Speech Emotion Recognition project:\n",
    "\n",
    "- **CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset)**\n",
    "  - **Description**: An audio-visual dataset comprising 7,442 clips from 91 actors (48 male, 43 female) aged between 20 and 74, representing diverse ethnic backgrounds. Actors vocalized 12 sentences expressing six emotions: anger, disgust, fear, happiness, neutral, and sadness. Each clip has multiple ratings for audio-only, visual-only, and audio-visual presentations.\n",
    "  - **Link**: https://www.kaggle.com/datasets/ejlok1/cremad\n",
    "\n",
    "- **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)**\n",
    "  - **Description**: Comprises 7,356 files from 24 professional actors (12 male, 12 female) speaking two lexically-matched statements in a neutral North American accent. Speech includes eight emotions: neutral, calm, happy, sad, angry, fearful, surprise, and disgust, each at two intensity levels. Available in audio-only, video-only, and audio-visual formats.\n",
    "  - **Link**: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\n",
    "\n",
    "- **Berlin Emotional Database**\n",
    "  - **Description**: Contains 535 utterances from ten actors (five male, five female) expressing seven emotions: anger, boredom, disgust, fear, happiness, sadness, and neutral. Recorded at 48kHz and downsampled to 16kHz.\n",
    "  - **Link**: http://emodb.bilderbar.info/\n",
    "  - **Kaggle Link**: https://www.kaggle.com/datasets/piyushagni5/berlin-database-of-emotional-speech-emodb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFAKF_NON0U7"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/ec523/projects/teamSER/miniconda/envs/mamba-env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MKmygLkSNuoy"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "import random\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  9 19:02:39 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  |   00000000:18:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             43W /  300W |       4MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-16GB           On  |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             43W /  300W |       1MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-16GB           On  |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   60C    P0            180W /  300W |     536MiB /  16384MiB |     72%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-16GB           On  |   00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   50C    P0             62W /  300W |    1356MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    2   N/A  N/A          756636      C   python                                  532MiB |\n",
      "|    3   N/A  N/A          834727      C   ....10.12/install/bin/python3.10       1352MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mamba', 'Mamba2', 'MambaLMHeadModel', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'distributed', 'mamba_inner_fn', 'models', 'modules', 'ops', 'selective_scan_fn', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import mamba_ssm\n",
    "print(dir(mamba_ssm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING: cuda\n",
      "Mamba output shape: torch.Size([2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"USING: \" + device.type)\n",
    "\n",
    "# Define a tiny Mamba model and move it to the GPU\n",
    "model = Mamba(\n",
    "    d_model=16,  # embedding dimension\n",
    "    d_state=8,   # internal state dimension\n",
    "    d_conv=4,    # convolution dimension\n",
    "    expand=2,    # expansion factor\n",
    ").to(device)\n",
    "\n",
    "# Generate random input (batch_size, sequence_length, embedding_dim) and move it to the GPU\n",
    "x = torch.randn(2, 10, 16).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "print(\"Mamba output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary variables, assuming root directory is /projectnb/ec523/projects/teamSER folder\n",
    "DATA_PATH=\"AudioWAV/\"\n",
    "\n",
    "training_split=0.8\n",
    "testing_split=0.2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=False, target_length=160):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.target_length = target_length\n",
    "\n",
    "        # enumeration of emotions\n",
    "        self.emotion_map = {\n",
    "            \"ANG\": 0, \"DIS\": 1, \"FEA\": 2,\n",
    "            \"HAP\": 3, \"NEU\": 4, \"SAD\": 5\n",
    "        }\n",
    "\n",
    "        # Filter only valid files with known emotion labels\n",
    "        self.audio_files = [\n",
    "            f for f in os.listdir(data_dir)\n",
    "            if f.endswith('.wav') and f.split('_')[2] in self.emotion_map\n",
    "        ]\n",
    "\n",
    "        # Extract labels\n",
    "        self.strlabels = [f.split('_')[2] for f in self.audio_files]\n",
    "        self.labels = [self.emotion_map[label] for label in self.strlabels]\n",
    "\n",
    "        # Fixed transforms\n",
    "        self.sample_rate = 16000\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=2048,\n",
    "            hop_length=512,\n",
    "            n_mels=128\n",
    "        )\n",
    "        self.db_transform = T.AmplitudeToDB()\n",
    "\n",
    "        # Resampler reused for efficiency\n",
    "        self.resampler = T.Resample(orig_freq=48000, new_freq=self.sample_rate)  # Assume worst-case\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.audio_files[idx])\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        # Resample to 16kHz if needed\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resample = T.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "            waveform = resample(waveform)\n",
    "\n",
    "        # Convert stereo to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Normalize waveform\n",
    "        waveform = waveform - waveform.mean()\n",
    "\n",
    "        # Volume augmentation on waveform\n",
    "        if self.transform and random.random() < 0.5:\n",
    "            waveform = T.Vol(gain=(0.5, 1.5), gain_type=\"amplitude\")(waveform)\n",
    "\n",
    "        # Compute Mel spectrogram and convert to dB\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        mel_spec = self.db_transform(mel_spec)\n",
    "\n",
    "        # MinMax normalization to [0, 1]\n",
    "        mel_min = mel_spec.min()\n",
    "        mel_max = mel_spec.max()\n",
    "        mel_spec = (mel_spec - mel_min) / (mel_max - mel_min + 1e-6)\n",
    "\n",
    "        # Spectrogram-level augmentation\n",
    "        if self.transform:\n",
    "            if random.random() < 0.5:\n",
    "                mel_spec = T.FrequencyMasking(freq_mask_param=15)(mel_spec)\n",
    "            if random.random() < 0.5:\n",
    "                mel_spec = T.TimeMasking(time_mask_param=35)(mel_spec)\n",
    "\n",
    "        # Fix time dimension by padding or cropping\n",
    "        current_length = mel_spec.shape[-1]\n",
    "        if current_length < self.target_length:\n",
    "            pad_amount = self.target_length - current_length\n",
    "            mel_spec = F.pad(mel_spec, (0, pad_amount))\n",
    "        else:\n",
    "            mel_spec = mel_spec[:, :, :self.target_length]\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        # Remove channel dimension if needed (1, 128, T) -> (128, T)\n",
    "        mel_spec = mel_spec.squeeze(0)\n",
    "\n",
    "        return mel_spec, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function pads per batch so that every spectogram is the same dimension per batch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    spectrograms, labels = zip(*batch)\n",
    "    \n",
    "    max_length = max(spec.shape[1] for spec in spectrograms)\n",
    "\n",
    "    #pad spectrograms to match longest\n",
    "    spectrograms_padded = [torch.nn.functional.pad(spec, (0, max_length - spec.shape[1])) for spec in spectrograms]\n",
    "\n",
    "    # Convert list to tensor\n",
    "    spectrograms_padded = torch.stack(spectrograms_padded)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return spectrograms_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring dataset\n",
    "dataset = AudioDataset(DATA_PATH)\n",
    "\n",
    "#calculate training size and testing size\n",
    "train_size = int(dataset.__len__()*training_split)\n",
    "test_size = dataset.__len__()-train_size\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_set.dataset.transform = True\n",
    "test_set.dataset.transform = False\n",
    "\n",
    "#dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "#FINAL DIMENSIONS OF SPECS: BatchSize x 128 x MaxTimeLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.7834 | Val Loss: 1.7523 | Val Acc: 17.86%\n",
      "Epoch 2: Train Loss: 1.7337 | Val Loss: 1.7076 | Val Acc: 29.35%\n",
      "Epoch 3: Train Loss: 1.6784 | Val Loss: 1.6335 | Val Acc: 29.21%\n",
      "Epoch 4: Train Loss: 1.5606 | Val Loss: 1.5326 | Val Acc: 36.74%\n",
      "Epoch 5: Train Loss: 1.5035 | Val Loss: 1.4644 | Val Acc: 38.68%\n",
      "Epoch 6: Train Loss: 1.4868 | Val Loss: 1.4381 | Val Acc: 38.48%\n",
      "Epoch 7: Train Loss: 1.4705 | Val Loss: 1.4229 | Val Acc: 41.10%\n",
      "Epoch 8: Train Loss: 1.4516 | Val Loss: 1.4133 | Val Acc: 41.71%\n",
      "Epoch 9: Train Loss: 1.4399 | Val Loss: 1.4246 | Val Acc: 43.92%\n",
      "Epoch 10: Train Loss: 1.4149 | Val Loss: 1.4282 | Val Acc: 43.32%\n",
      "Epoch 11: Train Loss: 1.3965 | Val Loss: 1.3816 | Val Acc: 45.40%\n",
      "Epoch 12: Train Loss: 1.3914 | Val Loss: 1.4011 | Val Acc: 46.34%\n",
      "Epoch 13: Train Loss: 1.3815 | Val Loss: 1.4063 | Val Acc: 44.66%\n",
      "Epoch 14: Train Loss: 1.3729 | Val Loss: 1.3805 | Val Acc: 47.01%\n",
      "Epoch 15: Train Loss: 1.3577 | Val Loss: 1.3751 | Val Acc: 46.88%\n",
      "Epoch 16: Train Loss: 1.3636 | Val Loss: 1.4291 | Val Acc: 47.01%\n",
      "Epoch 17: Train Loss: 1.3547 | Val Loss: 1.3983 | Val Acc: 47.35%\n",
      "Epoch 18: Train Loss: 1.3425 | Val Loss: 1.3754 | Val Acc: 46.34%\n",
      "Epoch 19: Train Loss: 1.3427 | Val Loss: 1.3048 | Val Acc: 48.56%\n",
      "Epoch 20: Train Loss: 1.3315 | Val Loss: 1.3020 | Val Acc: 48.35%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm import Mamba\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import random\n",
    "\n",
    "class PureAudioMamba(nn.Module):  \n",
    "    def __init__(self, num_classes=6, d_model=256):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(128, d_model)\n",
    "        \n",
    "        self.mamba1 = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=16,\n",
    "            d_conv=4,\n",
    "            expand=2\n",
    "        )\n",
    "        self.mamba2 = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=16,\n",
    "            d_conv=4,\n",
    "            expand=2\n",
    "        )\n",
    "        \n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(d_model, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  \n",
    "        x = self.input_proj(x)  \n",
    "        \n",
    "        x = self.mamba1(x)\n",
    "        x = self.mamba2(x)\n",
    "        \n",
    "        attn_weights = self.attention_pool(x)\n",
    "        x = torch.sum(x * attn_weights, dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def train_model(train_loader, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PureAudioMamba(num_classes=6).to(device)  # Fixed class name\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                              max_lr=1e-3,\n",
    "                                              total_steps=len(train_loader)*100)\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for specs, labels in train_loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for specs, labels in test_loader:\n",
    "                specs, labels = specs.to(device), labels.to(device)\n",
    "                outputs = model(specs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "train_model(train_loader, test_loader)\n",
    "\n",
    "# NOTE: mamba uses roughly 3 * expand * d_modeel^2 parameters\n",
    "# NOTE: AdamW is stochastic optimization that modifes the\n",
    "# Adam optimizer by decoupling weight decay from gradient update\n",
    "# NOTE: could also try switching scheduler for cosine with warmup; or lower max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test how many Epochs is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Train Loss: 1.7303 | Train Acc: 22.12% | Val Loss: 1.6675 | Val Acc: 26.33%\n",
      "Epoch 002: Train Loss: 1.5496 | Train Acc: 33.93% | Val Loss: 1.4819 | Val Acc: 34.45%\n",
      "Epoch 003: Train Loss: 1.5505 | Train Acc: 35.71% | Val Loss: 1.4555 | Val Acc: 37.61%\n",
      "Epoch 004: Train Loss: 1.4787 | Train Acc: 37.44% | Val Loss: 1.4295 | Val Acc: 39.76%\n",
      "Epoch 005: Train Loss: 1.4563 | Train Acc: 39.58% | Val Loss: 1.4077 | Val Acc: 41.97%\n",
      "Epoch 006: Train Loss: 1.4305 | Train Acc: 40.92% | Val Loss: 1.3753 | Val Acc: 43.72%\n",
      "Epoch 007: Train Loss: 1.4165 | Train Acc: 41.95% | Val Loss: 1.3702 | Val Acc: 42.78%\n",
      "Epoch 008: Train Loss: 1.3881 | Train Acc: 43.66% | Val Loss: 1.3334 | Val Acc: 46.54%\n",
      "Epoch 009: Train Loss: 1.3653 | Train Acc: 44.73% | Val Loss: 1.3124 | Val Acc: 47.08%\n",
      "Epoch 010: Train Loss: 1.3493 | Train Acc: 45.39% | Val Loss: 1.3073 | Val Acc: 46.68%\n",
      "Epoch 011: Train Loss: 1.3311 | Train Acc: 46.14% | Val Loss: 1.3354 | Val Acc: 45.94%\n",
      "Epoch 012: Train Loss: 1.3279 | Train Acc: 47.02% | Val Loss: 1.2846 | Val Acc: 48.09%\n",
      "Epoch 013: Train Loss: 1.3086 | Train Acc: 46.97% | Val Loss: 1.2910 | Val Acc: 46.94%\n",
      "Epoch 014: Train Loss: 1.3046 | Train Acc: 47.66% | Val Loss: 1.2716 | Val Acc: 48.89%\n",
      "Epoch 015: Train Loss: 1.2812 | Train Acc: 48.90% | Val Loss: 1.3214 | Val Acc: 46.41%\n",
      "Epoch 016: Train Loss: 1.2842 | Train Acc: 49.44% | Val Loss: 1.2739 | Val Acc: 50.50%\n",
      "Epoch 017: Train Loss: 1.2700 | Train Acc: 49.29% | Val Loss: 1.2367 | Val Acc: 51.85%\n",
      "Epoch 018: Train Loss: 1.2666 | Train Acc: 50.16% | Val Loss: 1.2322 | Val Acc: 51.98%\n",
      "Epoch 019: Train Loss: 1.2563 | Train Acc: 50.66% | Val Loss: 1.2475 | Val Acc: 51.18%\n",
      "Epoch 020: Train Loss: 1.2360 | Train Acc: 50.81% | Val Loss: 1.2058 | Val Acc: 53.39%\n",
      "Epoch 021: Train Loss: 1.2390 | Train Acc: 50.88% | Val Loss: 1.2211 | Val Acc: 51.31%\n",
      "Epoch 022: Train Loss: 1.2227 | Train Acc: 51.79% | Val Loss: 1.2178 | Val Acc: 52.05%\n",
      "Epoch 023: Train Loss: 1.2306 | Train Acc: 51.30% | Val Loss: 1.3212 | Val Acc: 46.94%\n",
      "Epoch 024: Train Loss: 1.2071 | Train Acc: 52.54% | Val Loss: 1.2350 | Val Acc: 50.24%\n",
      "Epoch 025: Train Loss: 1.2023 | Train Acc: 52.33% | Val Loss: 1.2029 | Val Acc: 50.97%\n",
      "No improvement for 5 epochs. Early stopping...\n",
      "Early stopping at epoch 26\n",
      "\n",
      "Training complete. Best validation accuracy: 53.39% at epoch 20\n"
     ]
    }
   ],
   "source": [
    "class PureAudioMamba(nn.Module):  \n",
    "    def __init__(self, num_classes=6, d_model=256):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(128, d_model)\n",
    "        self.mamba1 = Mamba(d_model=d_model, d_state=16, d_conv=4, expand=2)\n",
    "        self.mamba2 = Mamba(d_model=d_model, d_state=16, d_conv=4, expand=2)\n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(d_model, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.mamba1(x)\n",
    "        x = self.mamba2(x)\n",
    "        attn_weights = self.attention_pool(x)\n",
    "        x = torch.sum(x * attn_weights, dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def train_model(train_loader, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PureAudioMamba(num_classes=6).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_epoch = 0\n",
    "    patience = 5  # Stop after 5 epochs without improvement\n",
    "    early_stop = False\n",
    "    \n",
    "    for epoch in range(100):  # Max epochs set high for early stopping\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for specs, labels in train_loader:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for specs, labels in test_loader:\n",
    "                specs, labels = specs.to(device), labels.to(device)\n",
    "                outputs = model(specs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1:03d}: \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "        elif (epoch + 1 - best_epoch) >= patience:\n",
    "            print(f\"No improvement for {patience} epochs. Early stopping...\")\n",
    "            early_stop = True\n",
    "            \n",
    "    print(f\"\\nTraining complete. Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "\n",
    "# Start training\n",
    "train_model(train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all models should accept inputs of differnet lengths (shouldn't have to worry about mamba)\n",
    "#we should look into using global adaptive pooling\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "\n",
    "MODELS THAT WE NEED TO MAKE:\n",
    "CNN-Transformer: Should we use a resnet on this? Would that be overkill? We could use a resnet\n",
    "    and train it ourselves (not sure if a pretrained resnet would be great)\n",
    "    \n",
    "Regular CNN: this will be our base model for comparison. We should play around with this, and\n",
    "    this should be the same kind of CNN that we use in our other models (i.e. resnet?)\n",
    "    \n",
    "Mamba Model: we should train a basic mamba model\n",
    "\n",
    "Mamba-CNN: we should incorporate a cnn with a mamba model\n",
    "\n",
    "Pretrained SOTA model: we should delcare a pretrained state of the art model and compare against that\n",
    "'''\n",
    "class Sequential_CNN_Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequential_CNN_Transformer, self).__init__()\n",
    "        \n",
    "        #declare layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(128, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(1) #added unsqueeze to the training function instead\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # flatten for the transformer (batch, sequence, features)\n",
    "        x = x.flatten(2)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # added this to fix dimensionality issues in train_model method\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "class Base_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Base_CNN, self).__init__()\n",
    "        \n",
    "        #declare layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((1,1))\n",
    "        \n",
    "        self.classifier1 = nn.Linear(256, 64)\n",
    "        self.classifier2 = nn.Linear(64, 6)\n",
    "        \n",
    "        self.residualConv = nn.Conv2d(1, 64, kernel_size=1, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        \n",
    "        residual = self.residualConv(residual)\n",
    "        \n",
    "        x = F.pad(x, (0, 1))\n",
    "        \n",
    "        residual = residual[:, :, :x.shape[2], :x.shape[3]]\n",
    "        \n",
    "        # print(f\"x shape: {x.shape}, res shape: {residual.shape}\")\n",
    "        \n",
    "        x = x + residual\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        #to fix dimensionality\n",
    "        x = self.max_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.classifier1(x)\n",
    "        x = self.classifier2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Base_CNN_Transformer(nn.Module):\n",
    "    def __init__(self, transformer_layers=2, n_heads=4, transformer_dim=256):\n",
    "        super(Base_CNN_Transformer, self).__init__()\n",
    "        \n",
    "        # CNN\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, transformer_dim, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(transformer_dim)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=n_heads, dropout=0.2)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
    "\n",
    "        # Classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6)\n",
    "        )\n",
    "        \n",
    "    def apply_layernorm(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        return nn.LayerNorm([C, H, W]).to(x.device)(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv1(x))))\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv2(x))))\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv3(x))))\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv4(x))))\n",
    "\n",
    "        # Prepare data\n",
    "        batch_size, channels, freq, time = x.shape\n",
    "        x = x.view(batch_size, channels, freq * time)\n",
    "        x = x.permute(2, 0, 1)  # (seq_length, batch_size, transformer_dim)\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = x.permute(1, 2, 0)  # (batch_size, transformer_dim, seq_length)\n",
    "        x = self.global_pool(x).squeeze(2)\n",
    "\n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 1, 128, 256])\n",
      "Output shape:  torch.Size([1, 6])\n",
      "tensor([[-0.0770, -0.0534, -0.1549,  0.0021,  0.2035, -0.2948]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_model = Base_CNN_Transformer()\n",
    "\n",
    "dummy_input = torch.randn(1, 128, 256).unsqueeze(1)\n",
    "\n",
    "output = dummy_model(dummy_input)\n",
    "\n",
    "print(\"Input shape: \", dummy_input.shape)\n",
    "print(\"Output shape: \", output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "11.6\n",
      "8302\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # PyTorch version\n",
    "print(torch.version.cuda)  # CUDA version\n",
    "print(torch.backends.cudnn.version())  # cuDNN version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = Sequential_CNN_Transformer().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, device, train_loader, num_epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {running_loss:.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # no need for gradients in testing\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # the class with the highest value is prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 308.6506\n",
      "Epoch 2/10 Loss: 286.0523\n",
      "Epoch 3/10 Loss: 274.7413\n",
      "Epoch 4/10 Loss: 269.3430\n",
      "Epoch 5/10 Loss: 261.6475\n",
      "Epoch 6/10 Loss: 255.7968\n",
      "Epoch 7/10 Loss: 252.8377\n",
      "Epoch 8/10 Loss: 249.7726\n",
      "Epoch 9/10 Loss: 247.1257\n",
      "Epoch 10/10 Loss: 243.8430\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, optimizer, criterion, device, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 40.16\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(trained_model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "# Set to 6 output classes\n",
    "resnet.fc = nn.Linear(in_features=512, out_features=6)\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 286.7953\n",
      "Epoch 2/10 Loss: 251.3834\n",
      "Epoch 3/10 Loss: 236.2239\n",
      "Epoch 4/10 Loss: 221.6431\n",
      "Epoch 5/10 Loss: 209.7092\n",
      "Epoch 6/10 Loss: 194.8534\n",
      "Epoch 7/10 Loss: 182.7004\n",
      "Epoch 8/10 Loss: 160.8685\n",
      "Epoch 9/10 Loss: 140.0472\n",
      "Epoch 10/10 Loss: 115.6093\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "resnet = train_model(resnet, optimizer, criterion, device, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 44.93\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(resnet, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Base_CNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss: 370.1472\n",
      "Epoch 2/20 Loss: 259.5503\n",
      "Epoch 3/20 Loss: 245.5657\n",
      "Epoch 4/20 Loss: 231.0189\n",
      "Epoch 5/20 Loss: 216.5164\n",
      "Epoch 6/20 Loss: 200.5484\n",
      "Epoch 7/20 Loss: 188.2512\n",
      "Epoch 8/20 Loss: 164.3314\n",
      "Epoch 9/20 Loss: 147.4764\n",
      "Epoch 10/20 Loss: 120.3046\n",
      "Epoch 11/20 Loss: 105.0427\n",
      "Epoch 12/20 Loss: 85.1477\n",
      "Epoch 13/20 Loss: 110.7600\n",
      "Epoch 14/20 Loss: 45.0547\n",
      "Epoch 15/20 Loss: 77.1016\n",
      "Epoch 16/20 Loss: 24.6913\n",
      "Epoch 17/20 Loss: 41.2945\n",
      "Epoch 18/20 Loss: 18.8090\n",
      "Epoch 19/20 Loss: 25.5523\n",
      "Epoch 20/20 Loss: 50.9678\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, criterion, device, train_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 54.67\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Base_CNN_Transformer().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 Loss: 320.6334\n",
      "Epoch 2/40 Loss: 300.9683\n",
      "Epoch 3/40 Loss: 290.2176\n",
      "Epoch 4/40 Loss: 285.7240\n",
      "Epoch 5/40 Loss: 288.5643\n",
      "Epoch 6/40 Loss: 282.3227\n",
      "Epoch 7/40 Loss: 280.6535\n",
      "Epoch 8/40 Loss: 279.6519\n",
      "Epoch 9/40 Loss: 282.0516\n",
      "Epoch 10/40 Loss: 277.3430\n",
      "Epoch 11/40 Loss: 278.3120\n",
      "Epoch 12/40 Loss: 275.5279\n",
      "Epoch 13/40 Loss: 273.4604\n",
      "Epoch 14/40 Loss: 276.0052\n",
      "Epoch 15/40 Loss: 272.2635\n",
      "Epoch 16/40 Loss: 269.8875\n",
      "Epoch 17/40 Loss: 279.0987\n",
      "Epoch 18/40 Loss: 266.9859\n",
      "Epoch 19/40 Loss: 270.6569\n",
      "Epoch 20/40 Loss: 268.8173\n",
      "Epoch 21/40 Loss: 265.4277\n",
      "Epoch 22/40 Loss: 262.2337\n",
      "Epoch 23/40 Loss: 261.1713\n",
      "Epoch 24/40 Loss: 256.5142\n",
      "Epoch 25/40 Loss: 261.9590\n",
      "Epoch 26/40 Loss: 260.8328\n",
      "Epoch 27/40 Loss: 251.1511\n",
      "Epoch 28/40 Loss: 246.6781\n",
      "Epoch 29/40 Loss: 247.1224\n",
      "Epoch 30/40 Loss: 246.4069\n",
      "Epoch 31/40 Loss: 241.2922\n",
      "Epoch 32/40 Loss: 234.1422\n",
      "Epoch 33/40 Loss: 231.8619\n",
      "Epoch 34/40 Loss: 231.2366\n",
      "Epoch 35/40 Loss: 230.4072\n",
      "Epoch 36/40 Loss: 220.6230\n",
      "Epoch 37/40 Loss: 217.3497\n",
      "Epoch 38/40 Loss: 218.7514\n",
      "Epoch 39/40 Loss: 219.5989\n",
      "Epoch 40/40 Loss: 207.0487\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, criterion, device, train_loader, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 42.38\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMd2XnadPDWOhFscMsWNATp",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
