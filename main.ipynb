{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JngnQaoBmIzK"
   },
   "source": [
    "# **Speech to Emotion Recognition**\n",
    "James Knee, Tyler Nguyen, Varsha Singh, Anish Sinha, Nathan Strahs\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Task ###\n",
    "Our task is to use a deep learning architecture to identify the underlying emotion given some English speaking audio, formally known as Speech to Emotion Recognition (SER). Identifying emotions from speech is hard enough for people, and it requires careful analysis over time. Emotional conveyance is also subjective; different speakers articulate emotions differently, implying variations in pitch, intensity, rhythm, and cadence. This task is also challenging due to the complexity of raw audio signals, so the data will require significant amounts of preprocessing. In the end, we would like our model to differentiate between anger, disgust, fear, happiness, sadness, and neutrality.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roRQN0FsmxCS"
   },
   "source": [
    "# Architecture Overview\n",
    "\n",
    "1. **Preprocessing**\n",
    "      - Normalize audio volume\n",
    "      - Convert audio to time-frequency representations like Spectograms\n",
    "2. **Feature Extraction via ResNet**\n",
    "      - Feed spectogram into Residual Network\n",
    "      - Retain extracted features by removing final classification layer in ResNet\n",
    "3. **Temporal Modeling via Transformer Encoder**\n",
    "      - Pass ResNet output to transformer and capture long-range dependencies and sequential relationships in the audio\n",
    "4. **Classification Layer**\n",
    "      - Apply a softmax layer to classify the output into one of six emotion categories: anger, disgust, fear, happiness, sadness, neutrality.\n",
    "\n",
    "Alternative Model: State Space Model (SSM) such as Mamba\n",
    "\n",
    "[insert diagram here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP5MMN9Gm4ZH"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Below are the datasets we will use for our Speech Emotion Recognition project:\n",
    "\n",
    "- **CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset)**\n",
    "  - **Description**: An audio-visual dataset comprising 7,442 clips from 91 actors (48 male, 43 female) aged between 20 and 74, representing diverse ethnic backgrounds. Actors vocalized 12 sentences expressing six emotions: anger, disgust, fear, happiness, neutral, and sadness. Each clip has multiple ratings for audio-only, visual-only, and audio-visual presentations.\n",
    "  - **Link**: https://www.kaggle.com/datasets/ejlok1/cremad\n",
    "\n",
    "- **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)**\n",
    "  - **Description**: Comprises 7,356 files from 24 professional actors (12 male, 12 female) speaking two lexically-matched statements in a neutral North American accent. Speech includes eight emotions: neutral, calm, happy, sad, angry, fearful, surprise, and disgust, each at two intensity levels. Available in audio-only, video-only, and audio-visual formats.\n",
    "  - **Link**: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\n",
    "\n",
    "- **Berlin Emotional Database**\n",
    "  - **Description**: Contains 535 utterances from ten actors (five male, five female) expressing seven emotions: anger, boredom, disgust, fear, happiness, sadness, and neutral. Recorded at 48kHz and downsampled to 16kHz.\n",
    "  - **Link**: http://emodb.bilderbar.info/\n",
    "  - **Kaggle Link**: https://www.kaggle.com/datasets/piyushagni5/berlin-database-of-emotional-speech-emodb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFAKF_NON0U7"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MKmygLkSNuoy"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary variables\n",
    "DATA_PATH=\"AudioWAV/\"\n",
    "\n",
    "training_split=0.8\n",
    "testing_split=0.2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a class to help preprocessing\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, target_length=128):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        self.audio_files = [f for f in os.listdir(data_dir) if f.endswith('.wav')]\n",
    "        \n",
    "        self.emotion_map = {\n",
    "            \"ANG\": 0,  # Anger\n",
    "            \"DIS\": 1,  # Disgust\n",
    "            \"FEA\": 2,  # Fear\n",
    "            \"HAP\": 3,  # Happy/Joy\n",
    "            \"NEU\": 4,  # Neutral\n",
    "            \"SAD\": 5   # Sad\n",
    "        }\n",
    "        \n",
    "        # extract the label (3rd sequence)\n",
    "        self.strlabels = [(file.split('_')[2]) for file in self.audio_files]\n",
    "        self.labels = [self.emotion_map.get(label, -1) for label in self.strlabels]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_length = target_length\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.audio_files[idx])\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        # Convert to Mel spectrogram\n",
    "        mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=1024, hop_length=512, n_mels=128)\n",
    "        mel_spec = mel_spec_transform(waveform)\n",
    "\n",
    "        # Normalize\n",
    "        # mel_spec = (mel_spec - mel_spec.mean()) / mel_spec.std()\n",
    "        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-6) # added 1e-6 to prevent division by 0\n",
    "\n",
    "        label = self.labels[idx]  # Assuming labels are already encoded as integers\n",
    "        return mel_spec.squeeze(0), torch.tensor(label, dtype=torch.long)\n",
    "        #return mel_spec, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function pads per batch so that every spectogram is the same dimension per batch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    spectrograms, labels = zip(*batch)\n",
    "    \n",
    "    max_length = max(spec.shape[1] for spec in spectrograms)\n",
    "\n",
    "    #pad spectrograms to match longest\n",
    "    spectrograms_padded = [torch.nn.functional.pad(spec, (0, max_length - spec.shape[1])) for spec in spectrograms]\n",
    "\n",
    "    # Convert list to tensor\n",
    "    spectrograms_padded = torch.stack(spectrograms_padded)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return spectrograms_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring dataset\n",
    "dataset = AudioDataset(DATA_PATH)\n",
    "\n",
    "#calculate training size and testing size\n",
    "train_size = int(dataset.__len__()*training_split)\n",
    "test_size = dataset.__len__()-train_size\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "#dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "#FINAL DIMENSIONS OF SPECS: BatchSize x 128 x MaxTimeLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all models should accept inputs of differnet lengths (shouldn't have to worry about mamba)\n",
    "#we should look into using global adaptive pooling\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "\n",
    "MODELS THAT WE NEED TO MAKE:\n",
    "CNN-Transformer: Should we use a resnet on this? Would that be overkill? We could use a resnet\n",
    "    and train it ourselves (not sure if a pretrained resnet would be great)\n",
    "    \n",
    "Regular CNN: this will be our base model for comparison. We should play around with this, and\n",
    "    this should be the same kind of CNN that we use in our other models (i.e. resnet?)\n",
    "    \n",
    "Mamba Model: we should train a basic mamba model\n",
    "\n",
    "Mamba-CNN: we should incorporate a cnn with a mamba model\n",
    "\n",
    "Pretrained SOTA model: we should delcare a pretrained state of the art model and compare against that\n",
    "'''\n",
    "# class Sequential_CNN_Transformer(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Sequential_CNN_Transformer, self).__init__()\n",
    "        \n",
    "#         #declare layers\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#         self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.classifier = nn.Linear(128, 6)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         #x = x.unsqueeze(1) #added unsqueeze to the training function instead\n",
    "\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)\n",
    "\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)\n",
    "\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.bn3(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)\n",
    "        \n",
    "#         # flatten for the transformer (batch, sequence, features)\n",
    "#         x = x.flatten(2)\n",
    "#         x = x.permute(0, 2, 1)\n",
    "        \n",
    "#         # added this to fix dimensionality issues in train_model method\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x = self.global_pool(x)\n",
    "#         x = x.squeeze(2)\n",
    "#         x = self.classifier(x)\n",
    "\n",
    "#         return x\n",
    "        \n",
    "class Base_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Base_CNN, self).__init__()\n",
    "        \n",
    "        #declare layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((1,1))\n",
    "        \n",
    "        self.classifier1 = nn.Linear(256, 64)\n",
    "        self.classifier2 = nn.Linear(64, 6)\n",
    "        \n",
    "        self.residualConv = nn.Conv2d(1, 64, kernel_size=1, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        residual = self.residualConv(residual)\n",
    "        \n",
    "        x = F.pad(x, (0, 1))\n",
    "        \n",
    "        residual = residual[:, :, :x.shape[2], :x.shape[3]]\n",
    "        \n",
    "        # print(f\"x shape: {x.shape}, res shape: {residual.shape}\")\n",
    "        \n",
    "        x = x + residual\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        #to fix dimensionality\n",
    "        x = self.max_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.classifier1(x)\n",
    "        x = self.classifier2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Base_CNN_Transformer(nn.Module):\n",
    "    def __init__(self, transformer_layers=2, n_heads=4, transformer_dim=256):\n",
    "        super(Base_CNN_Transformer, self).__init__()\n",
    "        \n",
    "        # CNN\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, transformer_dim, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(transformer_dim)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=n_heads, dropout=0.2)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
    "\n",
    "        # Classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6)\n",
    "        )\n",
    "        \n",
    "    def apply_layernorm(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        return nn.LayerNorm([C, H, W]).to(x.device)(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv1(x))))\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv2(x))))\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv3(x))))\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv4(x))))\n",
    "\n",
    "        # Prepare data\n",
    "        batch_size, channels, freq, time = x.shape\n",
    "        x = x.view(batch_size, channels, freq * time)\n",
    "        x = x.permute(2, 0, 1)  # (seq_length, batch_size, transformer_dim)\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = x.permute(1, 2, 0)  # (batch_size, transformer_dim, seq_length)\n",
    "        x = self.global_pool(x).squeeze(2)\n",
    "\n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 1, 128, 256])\n",
      "Output shape:  torch.Size([1, 6])\n",
      "tensor([[ 0.8136,  0.0493, -0.3068, -0.0818, -0.2507,  0.0861]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_model = Base_CNN_Transformer()\n",
    "\n",
    "dummy_input = torch.randn(1, 128, 256).unsqueeze(1)\n",
    "\n",
    "output = dummy_model(dummy_input)\n",
    "\n",
    "print(\"Input shape: \", dummy_input.shape)\n",
    "print(\"Output shape: \", output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "11.6\n",
      "8302\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # PyTorch version\n",
    "print(torch.version.cuda)  # CUDA version\n",
    "print(torch.backends.cudnn.version())  # cuDNN version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = Sequential_CNN_Transformer().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, device, train_loader, num_epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {running_loss:.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # no need for gradients in testing\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # the class with the highest value is prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 308.6506\n",
      "Epoch 2/10 Loss: 286.0523\n",
      "Epoch 3/10 Loss: 274.7413\n",
      "Epoch 4/10 Loss: 269.3430\n",
      "Epoch 5/10 Loss: 261.6475\n",
      "Epoch 6/10 Loss: 255.7968\n",
      "Epoch 7/10 Loss: 252.8377\n",
      "Epoch 8/10 Loss: 249.7726\n",
      "Epoch 9/10 Loss: 247.1257\n",
      "Epoch 10/10 Loss: 243.8430\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, optimizer, criterion, device, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 40.16\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(trained_model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "# Set to 6 output classes\n",
    "resnet.fc = nn.Linear(in_features=512, out_features=6)\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 286.7953\n",
      "Epoch 2/10 Loss: 251.3834\n",
      "Epoch 3/10 Loss: 236.2239\n",
      "Epoch 4/10 Loss: 221.6431\n",
      "Epoch 5/10 Loss: 209.7092\n",
      "Epoch 6/10 Loss: 194.8534\n",
      "Epoch 7/10 Loss: 182.7004\n",
      "Epoch 8/10 Loss: 160.8685\n",
      "Epoch 9/10 Loss: 140.0472\n",
      "Epoch 10/10 Loss: 115.6093\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "resnet = train_model(resnet, optimizer, criterion, device, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 44.93\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(resnet, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Base_CNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss: 370.1472\n",
      "Epoch 2/20 Loss: 259.5503\n",
      "Epoch 3/20 Loss: 245.5657\n",
      "Epoch 4/20 Loss: 231.0189\n",
      "Epoch 5/20 Loss: 216.5164\n",
      "Epoch 6/20 Loss: 200.5484\n",
      "Epoch 7/20 Loss: 188.2512\n",
      "Epoch 8/20 Loss: 164.3314\n",
      "Epoch 9/20 Loss: 147.4764\n",
      "Epoch 10/20 Loss: 120.3046\n",
      "Epoch 11/20 Loss: 105.0427\n",
      "Epoch 12/20 Loss: 85.1477\n",
      "Epoch 13/20 Loss: 110.7600\n",
      "Epoch 14/20 Loss: 45.0547\n",
      "Epoch 15/20 Loss: 77.1016\n",
      "Epoch 16/20 Loss: 24.6913\n",
      "Epoch 17/20 Loss: 41.2945\n",
      "Epoch 18/20 Loss: 18.8090\n",
      "Epoch 19/20 Loss: 25.5523\n",
      "Epoch 20/20 Loss: 50.9678\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, criterion, device, train_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 54.67\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Base_CNN_Transformer().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 Loss: 320.6334\n",
      "Epoch 2/40 Loss: 300.9683\n",
      "Epoch 3/40 Loss: 290.2176\n",
      "Epoch 4/40 Loss: 285.7240\n",
      "Epoch 5/40 Loss: 288.5643\n",
      "Epoch 6/40 Loss: 282.3227\n",
      "Epoch 7/40 Loss: 280.6535\n",
      "Epoch 8/40 Loss: 279.6519\n",
      "Epoch 9/40 Loss: 282.0516\n",
      "Epoch 10/40 Loss: 277.3430\n",
      "Epoch 11/40 Loss: 278.3120\n",
      "Epoch 12/40 Loss: 275.5279\n",
      "Epoch 13/40 Loss: 273.4604\n",
      "Epoch 14/40 Loss: 276.0052\n",
      "Epoch 15/40 Loss: 272.2635\n",
      "Epoch 16/40 Loss: 269.8875\n",
      "Epoch 17/40 Loss: 279.0987\n",
      "Epoch 18/40 Loss: 266.9859\n",
      "Epoch 19/40 Loss: 270.6569\n",
      "Epoch 20/40 Loss: 268.8173\n",
      "Epoch 21/40 Loss: 265.4277\n",
      "Epoch 22/40 Loss: 262.2337\n",
      "Epoch 23/40 Loss: 261.1713\n",
      "Epoch 24/40 Loss: 256.5142\n",
      "Epoch 25/40 Loss: 261.9590\n",
      "Epoch 26/40 Loss: 260.8328\n",
      "Epoch 27/40 Loss: 251.1511\n",
      "Epoch 28/40 Loss: 246.6781\n",
      "Epoch 29/40 Loss: 247.1224\n",
      "Epoch 30/40 Loss: 246.4069\n",
      "Epoch 31/40 Loss: 241.2922\n",
      "Epoch 32/40 Loss: 234.1422\n",
      "Epoch 33/40 Loss: 231.8619\n",
      "Epoch 34/40 Loss: 231.2366\n",
      "Epoch 35/40 Loss: 230.4072\n",
      "Epoch 36/40 Loss: 220.6230\n",
      "Epoch 37/40 Loss: 217.3497\n",
      "Epoch 38/40 Loss: 218.7514\n",
      "Epoch 39/40 Loss: 219.5989\n",
      "Epoch 40/40 Loss: 207.0487\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, criterion, device, train_loader, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 42.38\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMd2XnadPDWOhFscMsWNATp",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
