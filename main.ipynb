{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JngnQaoBmIzK"
   },
   "source": [
    "# **Speech to Emotion Recognition**\n",
    "James Knee, Tyler Nguyen, Varsha Singh, Anish Sinha, Nathan Strahs\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Task ###\n",
    "Our task is to use a deep learning architecture to identify the underlying emotion given some English speaking audio, formally known as Speech to Emotion Recognition (SER). Identifying emotions from speech is hard enough for people, and it requires careful analysis over time. Emotional conveyance is also subjective; different speakers articulate emotions differently, implying variations in pitch, intensity, rhythm, and cadence. This task is also challenging due to the complexity of raw audio signals, so the data will require significant amounts of preprocessing. In the end, we would like our model to differentiate between anger, disgust, fear, happiness, sadness, and neutrality.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roRQN0FsmxCS"
   },
   "source": [
    "# Architecture Overview\n",
    "\n",
    "1. **Preprocessing**\n",
    "      - Normalize audio volume\n",
    "      - Convert audio to time-frequency representations like Spectograms\n",
    "2. **Feature Extraction via ResNet**\n",
    "      - Feed spectogram into Residual Network\n",
    "      - Retain extracted features by removing final classification layer in ResNet\n",
    "3. **Temporal Modeling via Transformer Encoder**\n",
    "      - Pass ResNet output to transformer and capture long-range dependencies and sequential relationships in the audio\n",
    "4. **Classification Layer**\n",
    "      - Apply a softmax layer to classify the output into one of six emotion categories: anger, disgust, fear, happiness, sadness, neutrality.\n",
    "\n",
    "Alternative Model: State Space Model (SSM) such as Mamba\n",
    "\n",
    "[insert diagram here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP5MMN9Gm4ZH"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Below are the datasets we will use for our Speech Emotion Recognition project:\n",
    "\n",
    "- **CREMA-D (Crowd-Sourced Emotional Multimodal Actors Dataset)**\n",
    "  - **Description**: An audio-visual dataset comprising 7,442 clips from 91 actors (48 male, 43 female) aged between 20 and 74, representing diverse ethnic backgrounds. Actors vocalized 12 sentences expressing six emotions: anger, disgust, fear, happiness, neutral, and sadness. Each clip has multiple ratings for audio-only, visual-only, and audio-visual presentations.\n",
    "  - **Link**: https://www.kaggle.com/datasets/ejlok1/cremad\n",
    "\n",
    "- **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)**\n",
    "  - **Description**: Comprises 7,356 files from 24 professional actors (12 male, 12 female) speaking two lexically-matched statements in a neutral North American accent. Speech includes eight emotions: neutral, calm, happy, sad, angry, fearful, surprise, and disgust, each at two intensity levels. Available in audio-only, video-only, and audio-visual formats.\n",
    "  - **Link**: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\n",
    "\n",
    "- **Berlin Emotional Database**\n",
    "  - **Description**: Contains 535 utterances from ten actors (five male, five female) expressing seven emotions: anger, boredom, disgust, fear, happiness, sadness, and neutral. Recorded at 48kHz and downsampled to 16kHz.\n",
    "  - **Link**: http://emodb.bilderbar.info/\n",
    "  - **Kaggle Link**: https://www.kaggle.com/datasets/piyushagni5/berlin-database-of-emotional-speech-emodb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFAKF_NON0U7"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "MKmygLkSNuoy"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "import random\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary variables\n",
    "DATA_PATH=\"AudioWAV/\"\n",
    "\n",
    "training_split=0.8\n",
    "testing_split=0.2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=False, target_length=160):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.target_length = target_length\n",
    "\n",
    "        self.emotion_map = {\n",
    "            \"ANG\": 0, \"DIS\": 1, \"FEA\": 2,\n",
    "            \"HAP\": 3, \"NEU\": 4, \"SAD\": 5\n",
    "        }\n",
    "\n",
    "        # Filter only valid files with known emotion labels\n",
    "        self.audio_files = [\n",
    "            f for f in os.listdir(data_dir)\n",
    "            if f.endswith('.wav') and f.split('_')[2] in self.emotion_map\n",
    "        ]\n",
    "\n",
    "        # Extract labels\n",
    "        self.strlabels = [f.split('_')[2] for f in self.audio_files]\n",
    "        self.labels = [self.emotion_map[label] for label in self.strlabels]\n",
    "\n",
    "        # Fixed transforms\n",
    "        self.sample_rate = 16000\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=2048,\n",
    "            hop_length=512,\n",
    "            n_mels=128\n",
    "        )\n",
    "        self.db_transform = T.AmplitudeToDB()\n",
    "\n",
    "        # Resampler reused for efficiency\n",
    "        self.resampler = T.Resample(orig_freq=48000, new_freq=self.sample_rate)  # Assume worst-case\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.audio_files[idx])\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        # Resample to 16kHz if needed\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resample = T.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "            waveform = resample(waveform)\n",
    "\n",
    "        # Convert stereo to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Normalize waveform\n",
    "        waveform = waveform - waveform.mean()\n",
    "\n",
    "        # Volume augmentation on waveform\n",
    "        if self.transform and random.random() < 0.5:\n",
    "            waveform = T.Vol(gain=(0.5, 1.5), gain_type=\"amplitude\")(waveform)\n",
    "\n",
    "        # Compute Mel spectrogram and convert to dB\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        mel_spec = self.db_transform(mel_spec)\n",
    "\n",
    "        # MinMax normalization to [0, 1]\n",
    "        mel_min = mel_spec.min()\n",
    "        mel_max = mel_spec.max()\n",
    "        mel_spec = (mel_spec - mel_min) / (mel_max - mel_min + 1e-6)\n",
    "\n",
    "        # Spectrogram-level augmentation\n",
    "        if self.transform:\n",
    "            if random.random() < 0.5:\n",
    "                mel_spec = T.FrequencyMasking(freq_mask_param=15)(mel_spec)\n",
    "            if random.random() < 0.5:\n",
    "                mel_spec = T.TimeMasking(time_mask_param=35)(mel_spec)\n",
    "\n",
    "        # Fix time dimension by padding or cropping\n",
    "        current_length = mel_spec.shape[-1]\n",
    "        if current_length < self.target_length:\n",
    "            pad_amount = self.target_length - current_length\n",
    "            mel_spec = F.pad(mel_spec, (0, pad_amount))\n",
    "        else:\n",
    "            mel_spec = mel_spec[:, :, :self.target_length]\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        # Remove channel dimension if needed (1, 128, T) -> (128, T)\n",
    "        mel_spec = mel_spec.squeeze(0)\n",
    "\n",
    "        return mel_spec, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function pads per batch so that every spectogram is the same dimension per batch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    spectrograms, labels = zip(*batch)\n",
    "    \n",
    "    max_length = max(spec.shape[1] for spec in spectrograms)\n",
    "\n",
    "    #pad spectrograms to match longest\n",
    "    spectrograms_padded = [torch.nn.functional.pad(spec, (0, max_length - spec.shape[1])) for spec in spectrograms]\n",
    "\n",
    "    # Convert list to tensor\n",
    "    spectrograms_padded = torch.stack(spectrograms_padded)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return spectrograms_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring dataset\n",
    "dataset = AudioDataset(DATA_PATH)\n",
    "\n",
    "#calculate training size and testing size\n",
    "train_size = int(dataset.__len__()*training_split)\n",
    "test_size = dataset.__len__()-train_size\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_set.dataset.transform = True\n",
    "test_set.dataset.transform = False\n",
    "\n",
    "#dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "#FINAL DIMENSIONS OF SPECS: BatchSize x 128 x MaxTimeLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all models should accept inputs of differnet lengths (shouldn't have to worry about mamba)\n",
    "#we should look into using global adaptive pooling\n",
    "\n",
    "'''\n",
    "TODO: \n",
    "\n",
    "MODELS THAT WE NEED TO MAKE:\n",
    "CNN-Transformer: Should we use a resnet on this? Would that be overkill? We could use a resnet\n",
    "    and train it ourselves (not sure if a pretrained resnet would be great)\n",
    "    \n",
    "Regular CNN: this will be our base model for comparison. We should play around with this, and\n",
    "    this should be the same kind of CNN that we use in our other models (i.e. resnet?)\n",
    "    \n",
    "Mamba Model: we should train a basic mamba model\n",
    "\n",
    "Mamba-CNN: we should incorporate a cnn with a mamba model\n",
    "\n",
    "Pretrained SOTA model: we should delcare a pretrained state of the art model and compare against that\n",
    "'''\n",
    "        \n",
    "class Base_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Base_CNN, self).__init__()\n",
    "        \n",
    "        #declare layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((1,1))\n",
    "        \n",
    "        self.classifier1 = nn.Linear(256, 64)\n",
    "        self.classifier2 = nn.Linear(64, 6)\n",
    "        \n",
    "        self.residualConv = nn.Conv2d(1, 64, kernel_size=1, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        residual = self.residualConv(residual)\n",
    "        \n",
    "        x = F.pad(x, (0, 1))\n",
    "        \n",
    "        residual = residual[:, :, :x.shape[2], :x.shape[3]]\n",
    "        \n",
    "        # print(f\"x shape: {x.shape}, res shape: {residual.shape}\")\n",
    "        \n",
    "        x = x + residual\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        #to fix dimensionality\n",
    "        x = self.max_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.classifier1(x)\n",
    "        x = self.classifier2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "\n",
    "class Base_CNN_Transformer(nn.Module):\n",
    "    def __init__(self, transformer_layers=2, n_heads=4, transformer_dim=256, input_freq_bins=8):\n",
    "        super(Base_CNN_Transformer, self).__init__()\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, transformer_dim, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(transformer_dim)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Projection from D*F to D\n",
    "        self.project = nn.Linear(transformer_dim * input_freq_bins, transformer_dim)\n",
    "\n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=n_heads, dropout=0.2)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
    "        self.pos_encoder = PositionalEncoding(transformer_dim)\n",
    "\n",
    "        # Classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6)\n",
    "        )\n",
    "\n",
    "    def apply_layernorm(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        return nn.LayerNorm([C, H, W]).to(x.device)(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv1(x))))\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv2(x))))\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv3(x))))\n",
    "        x = self.relu(self.apply_layernorm(self.pool(self.conv4(x))))  # [B, D, F, T]\n",
    "\n",
    "        B, D, F, T = x.shape\n",
    "\n",
    "        # Rearrange for transformer: each time step is a token\n",
    "        x = x.permute(0, 3, 1, 2)         # [B, T, D, F]\n",
    "        x = x.reshape(B, T, D * F)        # [B, T, D*F]\n",
    "        x = self.project(x)               # [B, T, D]\n",
    "\n",
    "        # Transformer expects [T, B, D]\n",
    "        x = x.permute(1, 0, 2)            # [T, B, D]\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Back to [B, D, T] for pooling\n",
    "        x = x.permute(1, 2, 0)            # [B, D, T]\n",
    "        x = self.global_pool(x).squeeze(2)  # [B, D]\n",
    "\n",
    "        x = self.classifier(x)  # [B, 6]\n",
    "        return x\n",
    "\n",
    "class Base_CNN_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Base_CNN_GRU, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.residualConv = nn.Conv2d(1, 64, kernel_size=1, stride=2, padding=1)\n",
    "\n",
    "        # GRU expects input_size = 256 (channels), and sequence length = width\n",
    "        self.gru = nn.GRU(input_size=256*8, hidden_size=128, num_layers=1,\n",
    "                          batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.classifier1 = nn.Linear(128 * 2, 64)  # bidirectional\n",
    "        self.classifier2 = nn.Linear(64, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # x: [B, 1, 128, 256]\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        residual = self.residualConv(residual)\n",
    "        x = F.pad(x, (0, 1))  # pad width to align\n",
    "        residual = residual[:, :, :x.shape[2], :x.shape[3]]\n",
    "        x = x + residual\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.bn4(x)\n",
    "\n",
    "        # x: [B, 256, H, W] â€” after all CNN and pooling layers\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Reshape for GRU: treat W as time steps, and C*H as input features\n",
    "        x = x.permute(0, 3, 1, 2)  # [B, W, C, H]\n",
    "        x = x.contiguous().view(B, W, C * H)  # [B, W, C*H]\n",
    "\n",
    "        # Update GRU input size if needed\n",
    "        x, _ = self.gru(x)  # GRU input_size = C*H\n",
    "\n",
    "        x = x[:, -1, :]  # last time step\n",
    "        x = self.classifier1(x)\n",
    "        x = self.classifier2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([1, 1, 128, 256])\n",
      "Output shape:  torch.Size([1, 6])\n",
      "tensor([[-0.3893,  0.0243,  0.1863,  0.2853, -0.0108,  0.2773]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_model = Base_CNN_GRU()\n",
    "\n",
    "dummy_input = torch.randn(1, 128, 256).unsqueeze(1)\n",
    "\n",
    "output = dummy_model(dummy_input)\n",
    "\n",
    "print(\"Input shape: \", dummy_input.shape)\n",
    "print(\"Output shape: \", output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "11.6\n",
      "8302\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # PyTorch version\n",
    "print(torch.version.cuda)  # CUDA version\n",
    "print(torch.backends.cudnn.version())  # cuDNN version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, device, train_loader, num_epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {running_loss:.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # no need for gradients in testing\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # the class with the highest value is prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Base_CNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 277.8612\n",
      "Epoch 2/10 Loss: 242.1291\n",
      "Epoch 3/10 Loss: 227.7572\n",
      "Epoch 4/10 Loss: 206.3700\n",
      "Epoch 5/10 Loss: 193.8552\n",
      "Epoch 6/10 Loss: 172.9846\n",
      "Epoch 7/10 Loss: 155.0231\n",
      "Epoch 8/10 Loss: 135.0378\n",
      "Epoch 9/10 Loss: 126.9739\n",
      "Epoch 10/10 Loss: 103.0389\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, optimizer, criterion, device, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 60.71\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(trained_model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = torchvision.models.resnet18(weights=False)\n",
    "\n",
    "# Set to 6 output classes\n",
    "resnet.fc = nn.Linear(in_features=512, out_features=6)\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 267.1126\n",
      "Epoch 2/10 Loss: 233.0354\n",
      "Epoch 3/10 Loss: 216.8892\n",
      "Epoch 4/10 Loss: 201.1892\n",
      "Epoch 5/10 Loss: 187.5245\n",
      "Epoch 6/10 Loss: 176.7374\n",
      "Epoch 7/10 Loss: 162.0703\n",
      "Epoch 8/10 Loss: 144.9984\n",
      "Epoch 9/10 Loss: 123.7047\n",
      "Epoch 10/10 Loss: 106.9874\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "resnet = train_model(resnet, optimizer, criterion, device, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 56.35\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(resnet, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base CNN Training (Alternative Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Base_CNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss: 316.0492\n",
      "Epoch 2/20 Loss: 263.4397\n",
      "Epoch 3/20 Loss: 246.0173\n",
      "Epoch 4/20 Loss: 225.8965\n",
      "Epoch 5/20 Loss: 213.6423\n",
      "Epoch 6/20 Loss: 198.3253\n",
      "Epoch 7/20 Loss: 182.8854\n",
      "Epoch 8/20 Loss: 177.3281\n",
      "Epoch 9/20 Loss: 153.2265\n",
      "Epoch 10/20 Loss: 150.8936\n",
      "Epoch 11/20 Loss: 132.3498\n",
      "Epoch 12/20 Loss: 119.7052\n",
      "Epoch 13/20 Loss: 90.1852\n",
      "Epoch 14/20 Loss: 92.4093\n",
      "Epoch 15/20 Loss: 64.6251\n",
      "Epoch 16/20 Loss: 56.2356\n",
      "Epoch 17/20 Loss: 65.6004\n",
      "Epoch 18/20 Loss: 37.1610\n",
      "Epoch 19/20 Loss: 16.2058\n",
      "Epoch 20/20 Loss: 34.5939\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, criterion, device, train_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 56.68\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Base_CNN_Transformer().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss: 293.5340\n",
      "Epoch 2/20 Loss: 264.7619\n",
      "Epoch 3/20 Loss: 250.5074\n",
      "Epoch 4/20 Loss: 238.6807\n",
      "Epoch 5/20 Loss: 222.9583\n",
      "Epoch 6/20 Loss: 215.1919\n",
      "Epoch 7/20 Loss: 202.2527\n",
      "Epoch 8/20 Loss: 186.1087\n",
      "Epoch 9/20 Loss: 178.2047\n",
      "Epoch 10/20 Loss: 157.3877\n",
      "Epoch 11/20 Loss: 144.3840\n",
      "Epoch 12/20 Loss: 129.1506\n",
      "Epoch 13/20 Loss: 108.6973\n",
      "Epoch 14/20 Loss: 141.1226\n",
      "Epoch 15/20 Loss: 79.3764\n",
      "Epoch 16/20 Loss: 67.9324\n",
      "Epoch 17/20 Loss: 57.0533\n",
      "Epoch 18/20 Loss: 49.2521\n",
      "Epoch 19/20 Loss: 39.5618\n",
      "Epoch 20/20 Loss: 32.9790\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, criterion, device, train_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 63.13\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with GRU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Base_CNN_GRU().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 Loss: 277.9192\n",
      "Epoch 2/15 Loss: 235.7404\n",
      "Epoch 3/15 Loss: 211.3875\n",
      "Epoch 4/15 Loss: 190.6624\n",
      "Epoch 5/15 Loss: 168.0269\n",
      "Epoch 6/15 Loss: 143.8124\n",
      "Epoch 7/15 Loss: 116.5335\n",
      "Epoch 8/15 Loss: 104.6810\n",
      "Epoch 9/15 Loss: 56.9923\n",
      "Epoch 10/15 Loss: 29.1031\n",
      "Epoch 11/15 Loss: 21.3106\n",
      "Epoch 12/15 Loss: 7.2766\n",
      "Epoch 13/15 Loss: 11.1959\n",
      "Epoch 14/15 Loss: 17.6103\n",
      "Epoch 15/15 Loss: 45.1654\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, criterion, device, train_loader, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 59.57\n"
     ]
    }
   ],
   "source": [
    "acc = test_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy of the model: {acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMd2XnadPDWOhFscMsWNATp",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
